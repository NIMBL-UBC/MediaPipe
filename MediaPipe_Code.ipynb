{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NIMBL-UBC/MediaPipe/blob/main/MediaPipe_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rDcpv3tCXE9"
      },
      "outputs": [],
      "source": [
        "#Installing MediaPipe\n",
        "!pip install -q mediapipe==0.10.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the required libraries\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "#import mediapipe as mp\n",
        "#from mediapipe.tasks import python\n",
        "#from mediapipe.tasks.python import vision\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tabulate import tabulate\n",
        "from scipy.interpolate import InterpolatedUnivariateSpline\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from shutil import rmtree\n",
        "\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "mp_hands = mp.solutions.hands"
      ],
      "metadata": {
        "id": "O1l6fhV6CaHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Link your google drive where the files to be evaluated are stored\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "s-FoZFN4CaBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_frames_count(video_path, folder_path):\n",
        "    # Check if the folder exists, if not create it\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "\n",
        "    # Function to extract frames\n",
        "    def FrameCapture(path):\n",
        "        # Path to video file\n",
        "        vidObj = cv2.VideoCapture(path)\n",
        "        # Used as a counter variable\n",
        "        count = 0\n",
        "        # Checks whether frames were extracted\n",
        "        success = 1\n",
        "        while success:\n",
        "            # Frame is extracted\n",
        "            success, image = vidObj.read()\n",
        "            if not success:\n",
        "                break\n",
        "            # Frame is stored in the assigned folder with a specific number\n",
        "            cv2.imwrite(os.path.join(folder_path, \"frame%d.jpg\" % count), image)\n",
        "            count += 1\n",
        "        return count  # Return the value of 'count' when the function finishes\n",
        "\n",
        "    # Call the FrameCapture function to extract frames and get the value of 'count'\n",
        "    countf = FrameCapture(video_path)\n",
        "\n",
        "    # To check if the code can read files\n",
        "    testimg = \"frame1.jpg\"\n",
        "    image_path = os.path.join(folder_path, testimg)\n",
        "\n",
        "    # Check if the file exists\n",
        "    if not os.path.exists(image_path):\n",
        "        print(\"Image file does not exist\")\n",
        "    else:\n",
        "        # Read the image using cv2.imread()\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is not None:\n",
        "            print('The image can be read by the code')\n",
        "        else:\n",
        "            # Unable to read the image\n",
        "            print(\"Error reading the image\")\n",
        "\n",
        "    # Check if the image was read successfully\n",
        "    if image is not None:\n",
        "        print(\"The image exists, proceed with code\")\n",
        "    else:\n",
        "        # Unable to read the image\n",
        "        print(\"Error reading the image\")\n",
        "\n",
        "    # Return the value of 'count' as the output of the function\n",
        "    return countf"
      ],
      "metadata": {
        "id": "3q4waKtIwRKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_coords(folder_path, count):\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(count):\n",
        "        filename = f\"/frame{i}.jpg\"\n",
        "        frameloc = folder_path + filename # Path of folder where frames have been extracted\n",
        "        #print(frameloc)\n",
        "        image = cv2.imread(frameloc)\n",
        "        #if image is not None:\n",
        "          #print(\"yes queen\")\n",
        "        with mp_hands.Hands(\n",
        "              static_image_mode=True,\n",
        "              max_num_hands=1, #{change between 1 or 2 ONLY}\n",
        "              min_detection_confidence=0.5) as hands:\n",
        "              #print(image.shape)\n",
        "              results = hands.process(image)\n",
        "              #print(type(image))\n",
        "              results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "              # Print handedness and draw hand landmarks on the image.\n",
        "              #print('Handedness:', results.multi_handedness) #{uncomment this line to print left or right handedness}\n",
        "              image_height, image_width, _ = image.shape\n",
        "              annotated_image = image.copy()\n",
        "              if results is not None:\n",
        "                if results.multi_hand_landmarks is not None:\n",
        "                  for hand_landmarks in results.multi_hand_landmarks:\n",
        "                  #print('hand_landmarks:', hand_landmarks) #{uncomment this line to print ALL landmarks for a single frame}\n",
        "                    print(\".\"\n",
        "                        #f'Index finger tip coordinates: (',\n",
        "                        #f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width}, '\n",
        "                        #f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * image_height})'\n",
        "                    )\n",
        "                  x.append(hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width)\n",
        "                  y.append(hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * image_height)\n",
        "              else:\n",
        "                #print(\"No hand landmarks found.\")\n",
        "                continue\n",
        "    shutil.rmtree(folder_path)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "9rfLuK30w2Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def std_data(x, y, vn):\n",
        "    table = list(zip(x, y))\n",
        "    df = pd.DataFrame(table, columns=[f'x{vn}', f'y{vn}'])\n",
        "    existing_rows = len(df[f'x{vn}'])\n",
        "    zero_rows = 1500 - existing_rows\n",
        "    additional_zeros = pd.DataFrame({f'x{vn}': np.zeros(zero_rows),\n",
        "                                     f'y{vn}': np.zeros(zero_rows)})\n",
        "    df = pd.concat([df, additional_zeros], ignore_index=True)\n",
        "\n",
        "    time = np.linspace(1, len(x), num=len(x))\n",
        "    nintp = 1500 - len(x)\n",
        "    newx = []\n",
        "    splinex = InterpolatedUnivariateSpline(time, x, k=3)\n",
        "    newx = splinex(np.linspace(time.min(), time.max(), nintp))\n",
        "\n",
        "    newy = []\n",
        "    spliney = InterpolatedUnivariateSpline(time, y, k=3)\n",
        "    newy = spliney(np.linspace(time.min(), time.max(), nintp))\n",
        "\n",
        "    joined_x = np.concatenate((x, newx))\n",
        "    joined_y = np.concatenate((y, newy))\n",
        "\n",
        "    df[f'resx{vn}'] = joined_x\n",
        "    df[f'resy{vn}'] = joined_y\n",
        "\n",
        "    #normx = scale(joined_x, with_mean=True, with_std=True)\n",
        "    #normy = scale(joined_y, with_mean=True, with_std=True)\n",
        "\n",
        "    scaler_x = MinMaxScaler()\n",
        "    scaler_y = MinMaxScaler()\n",
        "\n",
        "    joined_x_reshaped = joined_x.reshape(-1, 1)\n",
        "    joined_y_reshaped = joined_y.reshape(-1, 1)\n",
        "\n",
        "    normx = scaler_x.fit_transform(joined_x_reshaped)\n",
        "    normy = scaler_y.fit_transform(joined_y_reshaped)\n",
        "\n",
        "    df[f'normx{vn}'] = normx\n",
        "    df[f'normy{vn}'] = normy\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "S1W_Au4Lu3Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Path to video location, and the location of the folder where you want the frames to be extracted\n",
        "vpath = '/content/drive/MyDrive/videos/'\n",
        "fpath = '/content/drive/MyDrive/output_frames_'\n",
        "ext = \".mp4\"\n",
        "df_final = pd.DataFrame()\n",
        "df = pd.DataFrame()\n",
        "\n",
        "#trials = [\"1691\",\"1692\",\"1693\",\"1694\",\"1695\",\"1696\",\"1697\",\"1698\",\"1699\",\"16910\",\"1711\",\"1712\",\"1713\",\"1714\",\"1715\",\"1716\",\"1717\",\"1718\",\"1719\",\"17110\",\"1731\",\"1732\",\"1733\",\"1734\",\"1735\",\"1736\",\"1737\",\"1738\",\"1739\",\"17310\"]\n",
        "trials = [\"1281\",\"1282\",\"1283\",\"1284\",\"1285\",\"1286\",\"1287\",\"1288\",\"1289\",\"12810\"]\n",
        "#\"1121\",\"1122\",\"1123\",\"1124\",\"1125\",\"1126\",\"1127\",\"1128\",\"1129\",\"11210\",\"1251\",\"1252\",\"1253\",\"1254\",\"1255\",\"1256\",\"1257\",\"1258\",\"1259\",\"12510\",\"1321\",\"1322\",\"1323\",\"1324\",\"1325\",\"1326\",\"1327\",\"1328\",\"1329\",\"13210\",\"1341\",\"1342\",\"1343\",\"1344\",\"1345\",\"1346\",\"1347\",\"1348\",\"1349\",\"13410\",\"1381\",\"1382\",\"1383\",\"1384\",\"1385\",\"1386\",\"1387\",\"1388\",\"1389\",\"13810\",\"1401\",\"1402\",\"1403\",\"1404\",\"1405\",\"1406\",\"1407\",\"1408\",\"1409\",\"14010\",\"1411\",\"1412\",\"1413\",\"1414\",\"1415\",\"1416\",\"1417\",\"1418\",\"1419\",\"14110\",\"1431\",\"1432\",\"1433\",\"1434\",\"1435\",\"1436\",\"1437\",\"1438\",\"1439\",\"14310\",\"1691\",\"1692\",\"1693\",\"1694\",\"1695\",\"1696\",\"1697\",\"1698\",\"1699\",\"16910\",\"1711\",\"1712\",\"1713\",\"1714\",\"1715\",\"1716\",\"1717\",\"1718\",\"1719\",\"17110\",\"1731\",\"1732\",\"1733\",\"1734\",\"1735\",\"1736\",\"1737\",\"1738\",\"1739\",\"17310\"\n",
        "\n",
        "for trial in trials:\n",
        "  video_path = os.path.join(vpath + trial + ext)\n",
        "  folder_path = os.path.join(fpath + trial)\n",
        "  vn = trial\n",
        "  countt=get_frames_count(video_path, folder_path)\n",
        "  x,y = extract_coords(folder_path,countt)\n",
        "  df = std_data(x,y,vn)\n",
        "  df_final = pd.concat([df_final, df], axis =1)\n",
        "\n",
        "print(df_final)"
      ],
      "metadata": {
        "id": "8qsquFJcC5Bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.to_excel('MediaPipeData_128.xlsx', index=False)"
      ],
      "metadata": {
        "id": "2-rkZRx1-YCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final"
      ],
      "metadata": {
        "id": "xZpEvTwruAHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_excel('MediaPipeData_1251to13810series.xlsx')\n",
        "df2 = pd.read_excel('MediaPipeData_1401to1435series.xlsx')\n",
        "df3 = pd.read_excel('MediaPipeData_1691to17310series.xlsx')\n",
        "df_expo= pd.concat([df1, df2, df3], axis =1)\n",
        "df_expo\n",
        "df_expo.to_excel('MediapipeJPdata.xlsx', index=False)"
      ],
      "metadata": {
        "id": "zB-VYOxr7l3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QPdb3uupbbrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "path = r\"/content/drive/MyDrive\"\n",
        "dfTC_final = pd.DataFrame()\n",
        "\n",
        "def tlc_extract(id, s, t, vt):\n",
        "    tracelabdata = pd.read_csv(path + \"/tracings_2023VWupdate.txt\", sep=\"\\t\")\n",
        "    temp = tracelabdata[tracelabdata['id'] == id]\n",
        "    plotDat = temp[temp['session'] == s]\n",
        "    trialcheck = plotDat[plotDat['trial'] == t]\n",
        "    dfTC = trialcheck[['x', 'y','trial']]\n",
        "    dfTC = std_data(dfTC['x'],dfTC['y'], vt)\n",
        "    #column_names = [f'tlcx{vt}', f'tlcy{vt}', f'trial{vt}']\n",
        "    #dfTC.columns = column_names\n",
        "    dfTC.reset_index(drop=True, inplace=True)\n",
        "    return dfTC"
      ],
      "metadata": {
        "id": "N51D0BB5YRFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfTC = pd.DataFrame()\n",
        "dfTC_final = pd.DataFrame()\n",
        "dfTC_finalest = pd.DataFrame()"
      ],
      "metadata": {
        "id": "S5Ty5uSTrJpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id = [26, 28, 31, 32, 33, 33, 32, 35, 36, 37, 38, 39]\n",
        "s = [3, 4, 3, 1, 1, 4, 4, 1, 1, 1, 1, 1]\n",
        "t = [11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
        "\n",
        "vt = [\"1121\",\"1122\",\"1123\",\"1124\",\"1125\",\"1126\",\"1127\",\"1128\",\"1129\",\"11210\",\"1251\",\"1252\",\"1253\",\"1254\",\"1255\",\"1256\",\"1257\",\"1258\",\"1259\",\"12510\",\"1281\",\"1282\",\"1283\",\"1284\",\"1285\",\"1286\",\"1287\",\"1288\",\"1289\",\"12810\",\"1321\",\"1322\",\"1323\",\"1324\",\"1325\",\"1326\",\"1327\",\"1328\",\"1329\",\"13210\",\"1341\",\"1342\",\"1343\",\"1344\",\"1345\",\"1346\",\"1347\",\"1348\",\"1349\",\"13410\",\"1381\",\"1382\",\"1383\",\"1384\",\"1385\",\"1386\",\"1387\",\"1388\",\"1389\",\"13810\",\"1401\",\"1402\",\"1403\",\"1404\",\"1405\",\"1406\",\"1407\",\"1408\",\"1409\",\"14010\",\"1411\",\"1412\",\"1413\",\"1414\",\"1415\",\"1416\",\"1417\",\"1418\",\"1419\",\"14110\",\"1431\",\"1432\",\"1433\",\"1434\",\"1435\",\"1691\",\"1692\",\"1693\",\"1694\",\"1695\",\"1696\",\"1697\",\"1698\",\"1699\",\"16910\",\"1711\",\"1712\",\"1713\",\"1714\",\"1715\",\"1716\",\"1717\",\"1718\",\"1719\",\"17110\",\"1731\",\"1732\",\"1733\",\"1734\",\"1735\",\"1736\",\"1737\",\"1738\",\"1739\",\"17310\"]\n",
        "import numpy as np\n",
        "j = 9\n",
        "#n = 0\n",
        "while j < 12:\n",
        "  print(j)\n",
        "  for i in t:\n",
        "    print(id[j],s[j],i,vt[n])\n",
        "    dfTC = tlc_extract(id[j],s[j],i,vt[n])\n",
        "    dfTC_final = pd.concat([dfTC_final, dfTC], axis =1)\n",
        "    n = n+1\n",
        "  j=j+1\n",
        "\n",
        "print(dfTC_final)"
      ],
      "metadata": {
        "id": "su3mF6ywXPb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfTC_final.to_excel('TracelabDataforJP_final.xlsx', index=False)"
      ],
      "metadata": {
        "id": "pN17t-G7FFP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tlcx = dfTC[f'tlcx{vt}']\n",
        "tlcy = dfTC[f'tlcy{vt}']\n",
        "\n",
        "tlctime = np.linspace(1,len(dfTC[f'tlcx{vt}']),num=len(dfTC[f'tlcx{vt}']))\n",
        "\n",
        "tlcnintp = 250 - len(dfTC[f'tlcx{vt}'])  # Number of points for interpolation\n",
        "\n",
        "# Perform spline interpolation\n",
        "tlcnewx = []\n",
        "tlcsplinex = InterpolatedUnivariateSpline(tlctime, tlcx, k=3)\n",
        "tlcnewx = tlcsplinex(np.linspace(tlctime.min(), tlctime.max(), tlcnintp))\n",
        "\n",
        "tlcnewy=[]\n",
        "tlcspliney = InterpolatedUnivariateSpline(tlctime, tlcy, k=3)\n",
        "tlcnewy = tlcspliney(np.linspace(tlctime.min(), tlctime.max(), tlcnintp))\n",
        "\n",
        "print(tlcnewx)\n",
        "print(len(tlcnewx))"
      ],
      "metadata": {
        "id": "pwobuSCtAw-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tlcexisting_rows = len(dfTC[f'tlcx{vt}'])\n",
        "\n",
        "tlczero_rows = 250 - tlcexisting_rows\n",
        "\n",
        "# Create a DataFrame with the additional rows of zeros\n",
        "tlcadditional_zeros = pd.DataFrame({f'tlcx{vt}': np.zeros(tlczero_rows),\n",
        "                                 f'tlcx{vt}': np.zeros(tlczero_rows)})\n",
        "\n",
        "# Concatenate the existing DataFrame with the additional zeros DataFrame\n",
        "dfTC = pd.concat([dfTC, tlcadditional_zeros], ignore_index=True)\n"
      ],
      "metadata": {
        "id": "nXVCRC6KETwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joined_tlcx = np.concatenate((tlcx, tlcnewx))\n",
        "joined_tlcy = np.concatenate((tlcy, tlcnewy))\n",
        "\n",
        "dfTC[f'tlcresx{vt}'] = joined_tlcx\n",
        "dfTC[f'tlcresy{vt}'] = joined_tlcy\n",
        "\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "tlcnormx = scale(joined_tlcx, with_mean=True, with_std=True)\n",
        "tlcnormy = scale(joined_tlcy, with_mean=True, with_std=True)\n",
        "\n",
        "dfTC[f'tlcnormx{vt}'] = tlcnormx\n",
        "dfTC[f'tlcnormy{vt}'] = tlcnormy\n",
        "\n",
        "plt.scatter(tlcnormx, tlcnormy)\n",
        "len(tlcnormx)\n",
        "print(len(joined_tlcy))\n",
        "print(dfTC)\n"
      ],
      "metadata": {
        "id": "o4rLGFknAzlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_TLC = pd.concat([df_TLC, dfTC],axis = 1)\n",
        "print(df_TLC)"
      ],
      "metadata": {
        "id": "t83kzjo1fuT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_TLC.to_excel('TraceLabData.xlsx', index=False)"
      ],
      "metadata": {
        "id": "zeVWV2tokNNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To run procrutes analysis using a python library - not\n",
        "import numpy as np\n",
        "from scipy.spatial import procrustes\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have the 'MP' and 'TC' arrays defined in Python\n",
        "\n",
        "# Perform procrustes analysis\n",
        "mtx1, mtx2, disparity = procrustes(dfMP, dfTC)\n",
        "disparity\n",
        "\n",
        "# # Retrieve the results\n",
        "# PA = result[0]\n",
        "# disparity = result[1]\n",
        "\n",
        "# # Print summary\n",
        "# print(\"Translation:\", PA['translation'])\n",
        "# print(\"Rotation:\", PA['rotation'])\n",
        "# print(\"Scale:\", PA['scale'])\n",
        "# print(\"Disparity:\", disparity)\n",
        "\n",
        "# # Plot procrustes analysis results\n",
        "# plt.scatter(PA['source'][:, 0], PA['source'][:, 1], label='MP')\n",
        "# plt.scatter(PA['target'][:, 0], PA['target'][:, 1], label='TC')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "D3-1wN_lxdLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Generate 250 points for interpolation\n",
        "# interp_func = interp1d(np.arange(len(x)), x, kind='cubic')\n",
        "# #interp_funcy = interp1d(time, y, kind='cubic')\n",
        "# x_interp = interp_func(np.linspace(0, len(x) - 1, 250 * len(x)))\n",
        "# #y_new = interp_funcy(time)\n",
        "# x_new\n",
        "\n",
        "# from scipy.interpolate import BSpline, make_interp_spline\n",
        "# b = make_interp_spline(time, x)"
      ],
      "metadata": {
        "id": "ereD7dxY80tK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}